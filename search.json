[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Peter Bull",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nImplementing K-Means Clustering\n\n\n\n\n\n\n\nData Science\n\n\nAlgorithms\n\n\n\n\nImplementing a Simple K-Means Clustering Algorithm\n\n\n\n\n\n\nJun 9, 2023\n\n\nPeter Bull\n\n\n\n\n\n\n  \n\n\n\n\nLessons Learned & Creating a Stable Diffusion Pipeline\n\n\n\n\n\n\n\nDiffusion\n\n\n\n\nUnwrapping the hugging face stable diffusion pipeline a bit.\n\n\n\n\n\n\nMay 5, 2023\n\n\nPeter Bull\n\n\n\n\n\n\n  \n\n\n\n\nWelcome\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nApr 30, 2023\n\n\nPeter Bull\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Current BIM/Project Manager in the Architectural field(buildings, not bytes), aspiring AI dabbler."
  },
  {
    "objectID": "posts/230505-sd-pipeline/sd-pipeline.html",
    "href": "posts/230505-sd-pipeline/sd-pipeline.html",
    "title": "Lessons Learned & Creating a Stable Diffusion Pipeline",
    "section": "",
    "text": "a beautiful mountain\n\n\nHistorically, I’ve moved relatively quickly through any self-paced tutorials or classes that I’ve taken. I feel like I get the concepts, and can implement them in a few test cases, then it’s on to the next thing.  This hasn’t been an ideal approach.   About 3 years ago I convinced my boss to transition the firm from AutoCAD to Revit for all our projects, mostly so that I’d never have to type out a window schedule by hand again. Neither of us had ever touched it, and at the time the firm consisted of just myself and my boss.   I won’t bore you with the specifics, but after many tutorials and a decent amount of time wrestling with it, we had fully made the jump, and I was feeling really comfortable with my level of proficiency.   It wasn’t until we started training new hires to use it that I realized I had missed some foundational understanding of how the program worked under the hood. I could make it do the things I wanted, but I couldn’t quite explain why right off the bat.   Explaining it to someone else truly deepened my own comprehension and improved my workflow and understanding of what it could do significantly.   So, with that in mind I’m going to try to get ahead of that curve and do the same thing here. In future posts I hope to revisit and maybe refactor some of my old code from the first part of the course, but for the moment we’re going to hop in where I’m at:\n\nCreating a Stable Diffusion Pipeline from Components\n\n# !pip install -Uq diffusers transformers fastcore\n\n\nimport torch\nfrom transformers import CLIPTextModel, CLIPTokenizer\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\nfrom PIL import Image\n\nimport logging\n\n1logging.disable(logging.WARNING)\ntorch.manual_seed(1);\n\n\n1\n\nWhile quite helpful in any other situation, huggingface warnings will make this notebook fairly ugly, so for the time being, we’re disabling it.\n\n\n\n\n\nCLIP Tokenizer and Text Encoder\nFirst we’re going to load in a text_encoder and tokenizer. These are from the text portion of a CLIP model, and we’re going to use the weights available from OpenAI.  These will handle tokenizing our text and allow us to get the embeddings for our text prompt.\n\ntokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16)\ntext_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16).to(\"cuda\")\n\n\n\nVAE and UNET\nThe next step is to load in our vae and unet.\n\nfrom diffusers import AutoencoderKL, UNet2DConditionModel\n\n\nvae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-ema\", torch_dtype=torch.float16).to(\"cuda\")\nunet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", torch_dtype=torch.float16).to(\"cuda\")\n\n\n\nScheduling\nNext we’ll have to add a scheduler. The scheduler creates a schedule by: - Defining the number of noising steps - Defining the amount of noise added at each step \nThese are derived from the beta parameters, which we’ll pass in below  It’s important to note that when scheduling, we have to make sure to use the same noising schedule that was used during training\n\nfrom diffusers import LMSDiscreteScheduler\n\n\nscheduler = LMSDiscreteScheduler(num_train_timesteps=1000, beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\")\n\n\n\nParameters for Generation\nWe’re now going to define the parameters that we’ll use for generation\n\nprompt = [\"a photograph of a beautiful mountain landscape\"]\n\nheight = 512\nwidth = 512\nnum_inference_steps = 70\nguidance_scale = 7.5\nbatch_size = 1\n\n\n\nTokenizing the Prompt\nNext we’re going to tokenize the prompt using our tokenizer.   The model is going to require the same number of tokens for every prompt to optimize processing on the GPU. The GPU generally likes matrices to be the same size, so I have a feeling this is something that’s going to be a recurring theme for optimization.\n\ntext_input = tokenizer(\n    prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, \n    truncation=True, return_tensors=\"pt\"\n)\n\n\ntext_input['input_ids']\n\ntensor([[49406,   320,  8853,   539,   320,  1215,  3965,  5727, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407]])\n\n\nAbove are our tokens, with each number representing our input, with the exception of 49407, which is our padding\n\ntokenizer.decode(49407)\n\n'&lt;|endoftext|&gt;'\n\n\nAs illustrated above, our padding is just an ‘&lt;|endoftext|&gt;’ marker\nAnd if we look at the attention_mask, we can see that it’s doing its job by representing our padding tokens that we’re not interested in as 0’s\n\ntext_input['attention_mask']\n\ntensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0]])\n\n\n\n\nGetting Text Embeddings\nNow we can get the text embeddings for our prompt by using the text_encoder\n\ntext_embeddings = text_encoder(text_input.input_ids.to(\"cuda\"))[0].half()\ntext_embeddings.shape\n\ntorch.Size([1, 77, 768])\n\n\n\n\nGet Unconditioned Text Embeddings\nWe’re also going to get the embeddings needed to perform unconditional generation. Basically let the model do whatever it wants, so long as the end result is a decent-looking image.   We’re going to do this by simply using an empty string, \"\". The embeddings from this are what we’ll use in Classifier-Free Guidance\n\nmax_length = text_input.input_ids.shape[-1]\nuncond_input = tokenizer(\n    [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n)\n\nuncond_embeddings = text_encoder(uncond_input.input_ids.to(\"cuda\"))[0].half()\nuncond_embeddings.shape\n\ntorch.Size([1, 77, 768])\n\n\nThere will need to be two forward passes for Classifer-Free Guidance: 1. The conditioned input(text_embeddings) 1. The unconditioned input(uncond_embeddings)\nWe can actually roll them into one by concatenating both to a single batch:\n\n\nConcatenate Conditioned and Unconditioned Text Embeddings\n\ntext_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n\n\n\nCreate Initial Noise\nNext we’ll have to create our initial Gaussian noise, which will be our initial latents.\nOur latent shape is going to be 4x4x64, which will be decoded to our desired 3x512x512 after the denoising process is completed.\n\ntorch.manual_seed(255)\n\nlatents = torch.randn((batch_size, unet.in_channels, height // 8, width //8))\nlatents = latents.to(\"cuda\").half()\nlatents.shape\n\ntorch.Size([1, 4, 64, 64])\n\n\n\n\nInitializing the Scheduler and Scaling the Noise\nWe’re going to initialize our scheduler with the num_inference_steps we chose earlier\n\nscheduler.set_timesteps(num_inference_steps)\n\nAfter initializing the scheduler, we need to scale the initial noise by the standard deviation that’s required by the scheduler.\nThis values is going to change depending on what scheduler is used\n\nlatents = latents * scheduler.init_noise_sigma\n\n\n\nWriting the Denoising Loop\n\nscheduler.timesteps\n\ntensor([999.0000, 984.5217, 970.0435, 955.5652, 941.0870, 926.6087, 912.1304,\n        897.6522, 883.1739, 868.6957, 854.2174, 839.7391, 825.2609, 810.7826,\n        796.3043, 781.8261, 767.3478, 752.8696, 738.3913, 723.9130, 709.4348,\n        694.9565, 680.4783, 666.0000, 651.5217, 637.0435, 622.5652, 608.0870,\n        593.6087, 579.1304, 564.6522, 550.1739, 535.6957, 521.2174, 506.7391,\n        492.2609, 477.7826, 463.3043, 448.8261, 434.3478, 419.8696, 405.3913,\n        390.9130, 376.4348, 361.9565, 347.4783, 333.0000, 318.5217, 304.0435,\n        289.5652, 275.0870, 260.6087, 246.1304, 231.6522, 217.1739, 202.6957,\n        188.2174, 173.7391, 159.2609, 144.7826, 130.3043, 115.8261, 101.3478,\n         86.8696,  72.3913,  57.9130,  43.4348,  28.9565,  14.4783,   0.0000],\n       dtype=torch.float64)\n\n\nAs shown above, our scheduler has a specific schedule of steps going from 999 to 0(matching the 1000 steps that were used during training)\n\nscheduler.sigmas\n\ntensor([14.6146, 13.3974, 12.3033, 11.3184, 10.4301,  9.6279,  8.9020,  8.2443,\n         7.6472,  7.1044,  6.6102,  6.1594,  5.7477,  5.3709,  5.0258,  4.7090,\n         4.4178,  4.1497,  3.9026,  3.6744,  3.4634,  3.2680,  3.0867,  2.9183,\n         2.7616,  2.6157,  2.4794,  2.3521,  2.2330,  2.1213,  2.0165,  1.9180,\n         1.8252,  1.7378,  1.6552,  1.5771,  1.5031,  1.4330,  1.3664,  1.3030,\n         1.2427,  1.1852,  1.1302,  1.0776,  1.0272,  0.9788,  0.9324,  0.8876,\n         0.8445,  0.8029,  0.7626,  0.7236,  0.6858,  0.6490,  0.6131,  0.5781,\n         0.5438,  0.5102,  0.4770,  0.4443,  0.4118,  0.3795,  0.3470,  0.3141,\n         0.2805,  0.2455,  0.2084,  0.1672,  0.1174,  0.0292,  0.0000])\n\n\n\nplt.plot(scheduler.timesteps, scheduler.sigmas[:-1]);\n\n\n\n\n\n\nDenoising\n\nfor i, t in enumerate(tqdm(scheduler.timesteps)):\n    # create 2 latents, one for the text prompt, and one for the unconditioned prompt\n    input = torch.cat([latents] * 2)\n    # scale the noise on the latents\n    input = scheduler.scale_model_input(input, t)\n    \n    # predict the noise\n    with torch.no_grad(): pred = unet(input, t, encoder_hidden_states=text_embeddings).sample\n                                      \n    # perform guidance\n    pred_uncond, pred_text = pred.chunk(2)\n    pred = pred_uncond + guidance_scale * (pred_text - pred_uncond)\n    \n    # compute the \"previous\" noisy sample\n    latents = scheduler.step(pred, t, latents).prev_sample\n\n\n\n\n\n\nDecoding with the VAE\nThe latents now contain the denoised representation of the image, so all that’s left to do is use our vae to decode it to our pixel image\n\nwith torch.no_grad(): image = vae.decode(1 / 0.18215 * latents).sample\n\n\nwith torch.no_grad(): image_bad = vae.decode(1 / 0.125 * latents).sample\n\n\n\nDisplaying our Image with PIL\nNow we just have to do a bit of wrangling to put our images in a format that PIL can read, and we’re done!\n\nimage = (image / 2 + 0.5).clamp(0, 1)\nimage = image[0].detach().cpu().permute(1, 2, 0).numpy()\nimage = (image * 255).round().astype(\"uint8\")\nImage.fromarray(image)"
  },
  {
    "objectID": "posts/230430-welcome/welcome.html",
    "href": "posts/230430-welcome/welcome.html",
    "title": "Welcome",
    "section": "",
    "text": "Hello to anyone out there curious enough to read my blog! There are a lot of fancy bells and whistles on the internet, and I’m honored you’ve chosen my little chirp to spend your theoretical-attention-economy dollars on.\n\nSo, a little about me, and what I’m trying to accomplish here:\nI’m a currently a project and BIM manager for an Architecture firm. That generally involves designing the thing that’s going to get built, coordinating with the people who will actually build the thing that’s going to get built, making sure they use 1/2” Lag Bolts in the thing that’s going to get built, you get the idea.\nI actually don’t have any formal training in Architecture, but it was a career path that I wanted to persue after working for a few start-up companies that went belly up. It’s something I definitely enjoy, but anyone that works in the field will tell you that the tools of the trade are a bit clunky and outdated to say the least. The distance between what’s in your head and what’s construction-ready is a lot larger than I think it can or ought to be.\nEnter my girlfriend, who’s working on finishing her PhD, focused on machine learning and intentionality. She began working through the fastai deep learning for coders course at the recommendation of several colleagues and encouraged me to work through it alongside her.\nFast forward a couple months, and here we are! I’m currently working through the second half of the fastai course and am excited to continue exploring what deep learning has to offer. If I can utilize it in some way to narrow the chasm between brain and built, I’ll consider it a resounding success.\nFor now, I’m happy to just keep learning, and to share my journey with anyone that wants to follow along."
  },
  {
    "objectID": "posts/230609-k-means/k_means.html",
    "href": "posts/230609-k-means/k_means.html",
    "title": "Implementing K-Means Clustering",
    "section": "",
    "text": "This is my simple implementation of a K-Means Clustering algorithm, foregoing Within Cluster Sum of Squares and the Elbow Rule\nK-Means is a clustering algorithm that groups data as specified by the user (number of clusters, k). It assigns data points to the closest cluster based on the calculated distances.\nHere are the general steps: - Generate random centroids - Calculate the distance of each datapoint to each centroid - Assign each datapoint to the centroid it’s closest to - Recalculate the centroids as the mean of all datapoints assigned to a cluster - Repeat the cluster assignment and update until the algorithm converges\nfrom fastkaggle import get_dataset\nfrom pathlib import Path\nfrom fastai.tabular.all import *"
  },
  {
    "objectID": "posts/230609-k-means/k_means.html#dataset",
    "href": "posts/230609-k-means/k_means.html#dataset",
    "title": "Implementing K-Means Clustering",
    "section": "Dataset",
    "text": "Dataset\nMy first attempt at a clustering algorithm was a botched attempt at a DBSCAN from scratch. I wanted to use a dataset that was clearly segmented visually for ease of tracking what was happening. I haven’t quite worked out the kinks in the DBSCAN notebook yet, but I don’t see any harm in using it for k-means as well.\nThe data we’re going to target is the spending score as it relates to annual income\n\npath = Path('data/dbscan')\ndataset = 'vjchoudhary7/customer-segmentation-tutorial-in-python'\nif not path.exists():\n    data = get_dataset(path, dataset, unzip=True)\ndf = pd.read_csv(path/'Mall_Customers.csv', low_memory=False)\n\n\ndf.head()\n\n\n\n\n\n\n\n\nCustomerID\nGender\nAge\nAnnual Income (k$)\nSpending Score (1-100)\n\n\n\n\n0\n1\nMale\n19\n15\n39\n\n\n1\n2\nMale\n21\n15\n81\n\n\n2\n3\nFemale\n20\n16\n6\n\n\n3\n4\nFemale\n23\n16\n77\n\n\n4\n5\nFemale\n31\n17\n40"
  },
  {
    "objectID": "posts/230609-k-means/k_means.html#plotting-the-target-datapoints",
    "href": "posts/230609-k-means/k_means.html#plotting-the-target-datapoints",
    "title": "Implementing K-Means Clustering",
    "section": "Plotting the Target Datapoints",
    "text": "Plotting the Target Datapoints\n\nx = df['Annual Income (k$)']\ny = df['Spending Score (1-100)']\nplt.scatter(x,y)\nplt.xlabel('Annual Income (k$)')\nplt.ylabel('Spending Score (1-100)')\nplt.show()\n\n\n\n\nConvert the pandas dataframe to a pytorch tensor to utilize the GPU\n\nx_y_tens = torch.from_numpy(df.to_numpy()[:,3:].astype(float)).to(\"cuda\")"
  },
  {
    "objectID": "posts/230609-k-means/k_means.html#picking-k-centroids",
    "href": "posts/230609-k-means/k_means.html#picking-k-centroids",
    "title": "Implementing K-Means Clustering",
    "section": "Picking k Centroids",
    "text": "Picking k Centroids\nFirst, a random set of centroids is generated. This is the starting point for our clustering process. The centroids are the central points of our clusters. In this implementation, the number of centroids is chosen by a pretty clear visual grouping pattern in the data\n\nk = 5\n\nNext, we randomly pick those k random points as the initial cluster centroids\n\nsample_idxs = torch.randperm(x_y_tens.shape[0])[:5]\ncentroids = x_y_tens[sample_idxs]\ncentroids\n\ntensor([[60., 50.],\n        [63., 48.],\n        [38., 92.],\n        [73., 73.],\n        [88., 15.]], device='cuda:0', dtype=torch.float64)\n\n\nHere I’m just cloning the original centroids to use in a later animation\n\nog_centroids = centroids.clone()\n\nDetaching the tensors to numpy to plot gets pretty tedious, so here I’m creating a function to lighten the load just a bit.\n\ndef np_detach(tens):\n    return tens.detach().cpu().numpy()\n\nPlotting the initial data with centroids marked\n\nx = np_detach(x_y_tens)[:,0]\ny = np_detach(x_y_tens)[:,1]\n\nfig, ax = plt.subplots()\nax.scatter(x, y)\nfor cen in centroids:\n    ax.plot(np_detach(cen)[0], np_detach(cen)[1], markersize=10, marker=\"x\", color='r', mew=5)"
  },
  {
    "objectID": "posts/230609-k-means/k_means.html#calculating-the-distance-with-frobenius-norm",
    "href": "posts/230609-k-means/k_means.html#calculating-the-distance-with-frobenius-norm",
    "title": "Implementing K-Means Clustering",
    "section": "Calculating the Distance with Frobenius Norm",
    "text": "Calculating the Distance with Frobenius Norm\n\\[| A |{F} = \\sqrt{\\sum{i=1}^{m} \\sum_{j=1}^{n} |a_{ij}|^2} \\]\nThe Frobenius Norm is essentially the square root of the sum of the absolute squares of its elements. It’s essentially a way to “flatten” the matrix into a long vector, and then compute the Euclidean length of that vector.\nThis will allow us to calculate our distances by way of two tensors, rather than elementwise operation\n\ndef euclidean_dist(a, b):\n    return torch.norm(a-b, dim=-1)"
  },
  {
    "objectID": "posts/230609-k-means/k_means.html#calculate-distance-to-centroids-and-assign",
    "href": "posts/230609-k-means/k_means.html#calculate-distance-to-centroids-and-assign",
    "title": "Implementing K-Means Clustering",
    "section": "Calculate distance to centroids and assign",
    "text": "Calculate distance to centroids and assign\nNow we calculate the distance of each data point to each centroid\n\ndistances = euclidean_dist(x_y_tens.unsqueeze(1), centroids.unsqueeze(0))\n\nThen, assign each point to its nearest cluster\n\ncluster_assignments = torch.argmin(distances, dim=1)\n\nHere I’m just making another copy for later use in an animation\n\nog_cluster_assignments = cluster_assignments.clone()\n\nPlotting our initial cluster assignments:\n\ndef plot_clusters(data, centroids, assignments):\n    plt.clf()\n    data, centroids, assignments = np_detach(data), np_detach(centroids), np_detach(assignments)\n    plt.scatter(data[:,0], data[:,1], c=assignments, s=30)\n    plt.scatter(centroids[:,0], centroids[:,1], marker='x', color='red', s=60)\n\n\nplot_clusters(x_y_tens, centroids, cluster_assignments)\n\n\n\n\nWe’ll need to iterate several times over these steps, so we’ll start aggregating them into functions\n\ndef assign_to_nearest_centroid(data, centroids):\n    distances = euclidean_dist(data.unsqueeze(1), centroids.unsqueeze(0))\n    cluster_assignments = torch.argmin(distances, dim=1)\n    return cluster_assignments\n\nNow that we’ve assigned all datapoints to clusters, we have to update the centroids. This is done by taking the mean of all data points assigned to a cluster, resulting in a new centroid\n\ndef update_centroids(data, cluster_assignments, centroids): \n    new_centroids = torch.zeros_like(centroids)\n    for i in range(centroids.shape[0]):\n        assigned_points = data[cluster_assignments == i]\n        if len(assigned_points) &gt; 0:\n            new_centroids[i] = torch.mean(assigned_points, dim=0)\n    return new_centroids\n\n\nnew_centroids = update_centroids(x_y_tens, cluster_assignments, centroids)\n\nPlotting our centroid update:\n\nplot_clusters(x_y_tens, new_centroids, cluster_assignments)"
  },
  {
    "objectID": "posts/230609-k-means/k_means.html#repeat",
    "href": "posts/230609-k-means/k_means.html#repeat",
    "title": "Implementing K-Means Clustering",
    "section": "Repeat",
    "text": "Repeat\nNow we’ll run through the assignment/update loop several times and hopefully converge in the process\n\ndef k_means(data, centroids, iterations=10):\n    for i in range(iterations):\n        cluster_assignments = assign_to_nearest_centroid(data, centroids)\n        new_centroids = update_centroids(data, cluster_assignments, centroids)\n    cluster_assignments = assign_to_nearest_centroid(data, centroids)\n    return cluster_assignments, new_centroids\n\n\ncluster_assignments, centroids = k_means(x_y_tens, centroids)\n\n\nplot_clusters(x_y_tens, centroids, cluster_assignments)"
  },
  {
    "objectID": "posts/230609-k-means/k_means.html#animation",
    "href": "posts/230609-k-means/k_means.html#animation",
    "title": "Implementing K-Means Clustering",
    "section": "Animation",
    "text": "Animation\nFinally, we’ll use our original centroids and cluster assignments to see it in action\n\ncentroids = og_centroids\ncluster_assignments = og_cluster_assignments\n\n\nfrom matplotlib.animation import FuncAnimation\nfrom IPython.display import HTML\n\n\nfig, ax = plt.subplots()\n\n# Set up the initial plot\nscatter = ax.scatter(np_detach(x_y_tens[:, 0]), np_detach(x_y_tens[:, 1]), c=np_detach(cluster_assignments), cmap='viridis', alpha=0.5)\ncentroid_plot = ax.scatter(np_detach(centroids[:, 0]), np_detach(centroids[:, 1]), c='red', marker='x', s=100, label='Centroids')\nax.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f5a3824ecb0&gt;\n\n\n\n\n\n\ndef init():\n    return scatter, centroid_plot\n\n\ndef update(frame):\n    global x_y_tens, centroids, cluster_assignments\n    \n    # Step 1: Assign x_y_tens points to the nearest centroid\n    cluster_assignments = assign_to_nearest_centroid(x_y_tens, centroids)\n\n    # Step 2: Update centroids\n    updated_centroids = update_centroids(x_y_tens, cluster_assignments, centroids)\n\n    # Update plot\n    scatter.set_array(np_detach(cluster_assignments))\n    centroid_plot.set_offsets(np_detach(updated_centroids))\n    centroids = updated_centroids\n    \n    return scatter, centroid_plot\n\n\nani = FuncAnimation(fig, update, frames=10, init_func=init, blit=True, interval=200, repeat=False)\nplt.close()\nHTML(ani.to_jshtml())\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  }
]